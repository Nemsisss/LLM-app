# üíø Fred Again (product feature assessment and compliance evaluator tool)

## Table of Contents
[About](https://github.com/Nemsisss/Original-Product-LLM#about)

[Notion doc](https://github.com/Nemsisss/Original-Product-LLM#notion-doc)

[Project Structure](https://github.com/Nemsisss/Original-Product-LLM#project-structure)

[Architecture](https://github.com/Nemsisss/Original-Product-LLM#architecture)

[Run in Docker](https://github.com/Nemsisss/Original-Product-LLM#run-in-docker)

[Setup to run without docker](https://github.com/Nemsisss/Original-Product-LLM#setup-to-run-without-docker)
* [System dependencies](https://github.com/Nemsisss/Original-Product-LLM#setup-dependencies)
  
* [Clone the repository](https://github.com/Nemsisss/Original-Product-LLM#clone-this-repository)
* [Environment variables](https://github.com/Nemsisss/Original-Product-LLM#environment-variables)
* [Setup dependencies](https://github.com/Nemsisss/Original-Product-LLM#setup-dependencies)
  
[Input file requirements](https://github.com/Nemsisss/Original-Product-LLM#input-file-requirements)

[Switching to llama2](https://github.com/Nemsisss/Original-Product-LLM#switching-to-llama2)

[Run the app](https://github.com/Nemsisss/Original-Product-LLM#run-the-app)

[Next steps](https://github.com/Nemsisss/Original-Product-LLM#next-steps)

## About
This project was my summer 2023 internship project. The app is meant to help the sales/product teams in answering customer questions about the company's software product. The user uploads an input .csv file which contains the customer questions, and the app produces a .csv file which contains the responses, along with the sources of each response. Based on the responses, it also calculates a compliance score for the software product.

***Note:** Any sensitive information, API keys, links to the company and links to the company's data (including their gitbook and airtable url) has been removed. These can be replaced with any other product's gitbook and airtable url.*

I named this project Fred Again, as he is one of my favorite DJs and I had been listening to his songs during my internship period, so decided to name my project after him.
The purpose of the app is to help product/sales/marketing teams answer software-related questions; so questions like whether software allows ingesting data with XML sources and so on.

## Notion doc
You can find the Notion doc for this project at:
notion link 

## Project Structure 

| Folder/File  | Description  |
|---------|--------------|
| .streamlit | Contains streamlit's config file |
| Airtable_data | Contains data in JSON format pulled from Airtable table at airtable link|
| chroma_persist | Contains embedded Manual and Airtable data in chromadb (vector database)|
| ION-manual | Contains software's manual parsed from https://manual.company_name.io |
| responses | Contains responses and history CSV files generated by the app |
| rfps | Contains the RFPs input file in CSV format uploaded by the user |
| style | Contains the app's stylesheet |
| utils | Contains utility funcitons |
| /.env | Contains environment variables including OpenAI API key and Airtable API key |
| /ingest.py | Contains all the code for ingesting and parsing data and saving it to chromadb |
| /main.py | Contains the frontend main code |
| /model.py | Contains the model and langchain's chain logic |

## Architecture
<img width="1036" alt="architecture of the app" src="architecture.png">

## Run in Docker
Run the following commands in your terminal (make sure you're in the root directory) :
```
docker build -t streamlit .
```
```
docker run -p 8502:8502 streamlit
```
Wait for the docker to start running, then open a browser and navigate to: http://0.0.0.0:8502

## Setup to run without docker
If you want to run the app locally on your computer without using a docker container, follow the instructions:
### System dependencies:
* Python 3.11
* Python, pip, virtualenv (or manager of choice, such as pyenv, anaconda)

### Clone this repository:
`git@github.com:Nemsisss/Original-Product-LLM.git`

### Environment variables:
* Create a file named ".env" in the root directory
* Set the following environment variables:
  - `OPENAI_API_KEY` If you don't have one you should signup and generate an API key at https://openai.com/
  - `AT_TOKEN`
    - You need to request access for the Airtable
    - If you don't have one, you should signup and generate an API key at https://airtable.com/

### Setup dependencies:
```
pip install -U spacy
```
```
python -m spacy download en_core_web_sm
```
```
python -m spacy download en
```
```
pip install -r requirements.txt
```

## Input file requirements:
* Must be CSV file containing one column named "Prompt"
* Each prompt must start with a verb
  - "Does software" will be concatenated to the beginning of each prompt before being sent to the LLM, so structure each prompt in a way that when concatenated with "Does software", the question will make sense.
  - The following is an example of an acceptable csv input file:
  ```
  Prompt
  have management of user training for processes equipment and standard work,
  connect and communicate with Enterprise Resource Planning (ERP),
  allow push or pull data to cloud,
  ingest data with XML sources,
  ```

## Switching to Llama2:
* In the model.py file, inside the load_llm() function, uncomment all the lines that are commented out and take out or just comment out the API call for OpenAI
* In the model.py file, inside the retrieval_qa_chain() function, uncomment the HuggingFaceEmbeddings call, and remove or comment out the OpenAIEmbeddings call. Also, do not forget to uncomment the import for HuggingFaceEmbeddings that is at the very top of the model.py file. 
* In the ingest.py file, inside ingest_docs() fucntion, uncomment the HuggingFaceEmbeddings call, and remove or comment out the OpenAIEmbeddings call. Also, do not forget to uncomment the import for HuggingFaceEmbeddings that is at the very top of the ingest.py file. 
* We don't need the OPENAI_API_KEY in the .env file anymore, so that can be removed as well (in case you have added one).
* Inside the Dockefile, commands need to be changed in order to activate a specific GPU, visit https://python.langchain.com/docs/integrations/llms/llamacpp to find the appropriate command for the specific GPU you'll be using. Remember that METAL cannot be used for running the app in docker!

And that's it! it should be ready to go!

## Run the app
You can run the app using the following command:
```
streamlit run main.py
```
**Important things to note when running the app**:
* Once you upload a CSV file to start processing the prompts, please:
  - **DO NOT** switch the tabs on the left side bar, stay on the page until the program's done processing the file
  - **DO NOT** refresh the page
* Unfortunately streamlit (streamlit is the frontend used for this app) apps do not support persisting data between page refreshes, so if you refresh the page while you are on home page, you will lose all the prompts that have been processed. However, you should still be able to download the history file which will include the most recently processed prompts as well.
* If you select "Parse Data" from the side bar tabs and start parsing the manual/Airtable data, please:
  - **DO NOT** switch the tabs on the left side bar, stay on the page until the program's done parsing updated data from Manual and Airtable.
  - **DO NOT** refresh the page
 
## Next steps
- Currently, the app relies on the user to upload properly formatted CSV file of RFPs. However, in order to avoid manually changing/correcting every single prompt, we can have the program perform the manipulations using NLP (Natural language processing). NLP is currently being applied to the data pulled from Airtable, so a similar approach should work on the rfps input file as well. The rfps csv file should have one column only named prompt and each prompt should start with a verb. Following are some examples of expected format:
    - **have** management of user training for processes equipment and standard work,
    - **connect** and communicate with Enterprise Resource Planning (ERP),
- Explore AWS containers that come with GPUs (such as g5g.2xlarge) and replace OpenAI with llama2
    - The program code does not need to be changed, the lines of code that I wrote for using llama2 as LLM are still in the program, they just need to be uncommented and the code for calling OpenAI needs to be removed. The steps to follow for switching the model to llama2 are documented in the app‚Äôs Github repository‚Äôs ReadMe at https://github.com/company_name/ion-rfp-processor#switching-to-llama2.
    - After switching the model to llama2, you can start fine-tuning it to get more accurate responses.
- Add a feature to the app where a member of the team can approve the responses generated by the app/LLM and have the app add the prompt and its corresponding response to the ‚ÄúNew Prospective Customer Requirements List‚Äù Airtable table. This will help improve the quality and accuracy of answers for future prompt processing tasks.
- In case the model is switched to llama2, I would recommend changing the frontend to a more robust and complete frontend framework (such as React.js).
    - If using streamlit with llama2, you should consider the following limitations:
        - Once the user uploads an input file to process, the model cannot be terminated right away, it needs to finish processing the current prompt before it can terminate. The issue with this is that even though UI/frontend will show that the program is terminated, the model may still be running on the server/backend trying to finish processing the current prompt. If the user tries to refresh the page or click on a tab/button while the model has not stopped, both the server and the browser will crash. At the time of development, I could not find a way to signal the frontend that the model is still executing neither did I find a way to signal the model that it needs to stop executing right away.



